{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_NN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH1OKBgMwtNF",
        "colab_type": "code",
        "outputId": "2747c5b9-ba89-40c3-d79d-be9337ce2750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quFZmKGZxSVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "1# Some of the functions are stored in external files. \n",
        "# So that the loader could find those files, we need to explicitly show the path where they are stored\n",
        "# The path to files may differ in your case... \n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/AUA/AUA 2020 Spring/Deep Learning/DL_HW4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mes64tU6zRIi",
        "colab_type": "code",
        "outputId": "93a32a65-bc00-40b8-ba0a-89721c6cb42f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import torch\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "from testCases_v4a_Pytorch import *\n",
        "from dnn_utils_v2_Pytorch import sigmoid, sigmoid_backward, relu, relu_backward\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "torch.set_printoptions(precision=10) # shows 10 digits after the floating dot\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f295dd76390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdbusWUAzRIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \n",
        "    torch.manual_seed(1)\n",
        ")\n",
        "    W1 = torch.randn(n_h, n_x)*0.01\n",
        "    b1 = torch.zeros((n_h, 1))\n",
        "    W2 = torch.randn(n_y, n_h)*0.01\n",
        "    b2 = torch.zeros((n_y, 1))\n",
        "\n",
        "    assert(W1.shape == (n_h, n_x))\n",
        "    assert(b1.shape == (n_h, 1))\n",
        "    assert(W2.shape == (n_y, n_h))\n",
        "    assert(b2.shape == (n_y, 1))\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "jL-RTI14zRIp",
        "colab_type": "code",
        "outputId": "43194188-0bbc-46f3-b671-dc66559bfe99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "parameters = initialize_parameters(3,2,1)\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 = tensor([[ 0.0066135214,  0.0026692410,  0.0006167726],\n",
            "        [ 0.0062131733, -0.0045190598, -0.0016613023]])\n",
            "b1 = tensor([[0.],\n",
            "        [0.]])\n",
            "W2 = tensor([[-0.0152276848,  0.0038168391]])\n",
            "b2 = tensor([[0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuDq1l4HzRIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters_deep(layer_dims):\n",
        "    torch.manual_seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = torch.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
        "        parameters['b' + str(l)] = torch.zeros((layer_dims[l], 1))\n",
        "        \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOPrCOkdzRIv",
        "colab_type": "code",
        "outputId": "ae9e916a-073c-438d-f2b0-61ba84b8b69d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "parameters = initialize_parameters_deep([5,4,3])\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 = tensor([[-0.0007664429,  0.0035988151, -0.0078201676,  0.0007152773,\n",
            "          0.0104602277],\n",
            "        [ 0.0052218698, -0.0016680915,  0.0005303340,  0.0040460713,\n",
            "          0.0061129960],\n",
            "        [ 0.0076039038, -0.0003364326,  0.0099792238,  0.0045918045,\n",
            "          0.0243639704],\n",
            "        [-0.0014680817,  0.0048511401,  0.0020523437,  0.0033842819,\n",
            "          0.0135275396]])\n",
            "b1 = tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "W2 = tensor([[ 0.0049518901, -0.0016431698, -0.0067796218, -0.0105910739],\n",
            "        [ 0.0074769664,  0.0023891740, -0.0039215023,  0.0015191489],\n",
            "        [-0.0118373316,  0.0053436072, -0.0145102274, -0.0062937401]])\n",
            "b2 = tensor([[0.],\n",
            "        [0.],\n",
            "        [0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97WB-p8BzRIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_forward(A, W, b):\n",
        "\n",
        "    Z = W@A + b\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFBj71ejzRI1",
        "colab_type": "code",
        "outputId": "98ececce-0238-4d67-bf04-4d52eebee916",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "A, W, b = linear_forward_test_case()\n",
        "\n",
        "Z, linear_cache = linear_forward(A, W, b)\n",
        "print(\"Z = \" + str(Z))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Z = tensor([[-1.0822153091, -0.5616527200]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW2Z8LPKzRI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "\n",
        "    if activation == \"sigmoid\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knql58s4zRI7",
        "colab_type": "code",
        "outputId": "3d3137b9-6a60-477d-e795-a2d67d454ed6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "A_prev, W, b = linear_activation_forward_test_case()\n",
        "\n",
        "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
        "print(\"With sigmoid: A = \" + str(A))\n",
        "\n",
        "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
        "print(\"With ReLU: A = \" + str(A))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With sigmoid: A = tensor([[0.1978137195, 0.1715756804]])\n",
            "With ReLU: A = tensor([[0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNigNLgYzRI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_model_forward(X, parameters):\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    \n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
        "        caches.append(cache)\n",
        "  \n",
        "    \n",
        "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)],parameters['b' + str(L)], activation = \"sigmoid\")\n",
        "    caches.append(cache)\n",
        "    \n",
        "    assert(AL.shape == (1,X.shape[1]))\n",
        "            \n",
        "    return AL, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSHJV-nzzRJC",
        "colab_type": "code",
        "outputId": "846a487d-e39f-4230-ec43-737b26317f6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "X, parameters = L_model_forward_test_case_2hidden()\n",
        "AL, caches = L_model_forward(X, parameters)\n",
        "print(\"AL = \" + str(AL))\n",
        "print(\"Length of caches list = \" + str(len(caches)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AL = tensor([[0.7837043405, 0.7705814242, 0.5695444345, 0.7837043405]])\n",
            "Length of caches list = 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jow97bmrzRJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_cost(AL, Y):\n",
        "    m = Y.shape[1]\n",
        "    cost = (-1/m) * ((Y @ torch.log(AL).t() + (1 - Y) @ torch.log(1 - AL).t()))\n",
        "\n",
        "    \n",
        "    cost = torch.squeeze(cost)      # To make sure cost's shape is what we expect.\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN5kdDPMzRJI",
        "colab_type": "code",
        "outputId": "24735a2c-9139-40c6-c1d9-7ff6ccfe1fee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y, AL = compute_cost_test_case()\n",
        "print(\"cost = \" + str(compute_cost(AL, Y)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cost = tensor(0.2797765732)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQGaIz1OzRJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dA_prev = W.t()@dZ\n",
        "    dW = 1/m *(dZ @ A_prev.t()) \n",
        "    db =  1/m * dZ.sum(1, keepdim=True)                  \n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsZJiNp8zRJR",
        "colab_type": "code",
        "outputId": "7f3e8362-8937-4e7b-8650-3c12d5d2ab8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "dZ, linear_cache = linear_backward_test_case()\n",
        "\n",
        "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(dW))\n",
        "print (\"db = \" + str(db))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dA_prev = tensor([[-0.8316336274, -0.3802693784, -0.1003148183, -0.6336339712],\n",
            "        [-0.7257670760, -0.3817131817, -1.8475147486,  0.6139277816],\n",
            "        [ 0.8353895545,  0.4270242155,  0.9488694072,  0.0108328313],\n",
            "        [-2.0349035263, -0.9647823572,  0.2986161709, -1.7612611055],\n",
            "        [ 0.9973999858,  0.5078689456,  0.2876567841,  0.5071032047]])\n",
            "dW = tensor([[ 0.1632022560,  0.1284346581, -0.1838184297, -0.1737741232,\n",
            "         -0.0176431704],\n",
            "        [-0.7802008390, -0.1271579564,  0.0151971653, -0.0429767892,\n",
            "         -0.0452116542],\n",
            "        [-0.6705543399, -0.0343476534,  0.1530921608,  0.0376261622,\n",
            "         -0.1165488437]])\n",
            "db = tensor([[ 0.4028177261],\n",
            "        [-0.4397802055],\n",
            "        [-0.6353005171]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFfKdSCozRJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "      linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA,activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
        "                \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA,activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
        "            \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOhdJtfNzRJf",
        "colab_type": "code",
        "outputId": "d6869201-2b50-49bc-990b-edbb7dc23daf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
        "\n",
        "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
        "print (\"sigmoid:\")\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(dW))\n",
        "print (\"db = \" + str(db) + \"\\n\")\n",
        "\n",
        "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n",
        "print (\"relu:\")\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(dW))\n",
        "print (\"db = \" + str(db))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sigmoid:\n",
            "dA_prev = tensor([[-0.0254830271,  0.0152399382],\n",
            "        [-0.0677984580,  0.0405463725],\n",
            "        [ 0.0857409015, -0.0512767211]])\n",
            "dW = tensor([[0.0130424909, 0.0462693349, 0.0081148185]])\n",
            "db = tensor([[0.0130683035]])\n",
            "\n",
            "relu:\n",
            "dA_prev = tensor([[-0., 0.],\n",
            "        [-0., 0.],\n",
            "        [0., -0.]])\n",
            "dW = tensor([[0., 0., 0.]])\n",
            "db = tensor([[0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guWTdstTzRJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "   \n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (torch.div(Y, AL) - torch.div(1 - Y, 1 - AL))\n",
        "    \n",
        "\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
        "    \n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, activation = \"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kPVh1KYzRJl",
        "colab_type": "code",
        "outputId": "2a8b93a9-3ee6-46de-d4c7-4014bc462e10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "AL, Y_assess, caches = L_model_backward_test_case()\n",
        "grads = L_model_backward(AL, Y_assess, caches)\n",
        "print_grads(grads)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dW1 = tensor([[-0.0137194376, -0.0071208226, -0.0520380065,  0.2212887704],\n",
            "        [ 0.0295770243,  0.0153514119,  0.1121860445, -0.4770649970],\n",
            "        [-0.0151281655, -0.0337276235,  0.0249240343,  0.0240937192]])\n",
            "db1 = tensor([[-0.1541889310],\n",
            "        [ 0.3324079216],\n",
            "        [ 0.0246500261]])\n",
            "dA1 = tensor([[-0.3083778620,  0.2361355573],\n",
            "        [ 0.6648158431, -0.5090724230],\n",
            "        [-0.0643826947,  0.0493000522]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-USdTWP0zRJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\"+ str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\"+ str(l+1)]\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI0Wd5utzRJs",
        "colab_type": "code",
        "outputId": "21e7ca3b-47a5-40ec-ef99-a8d3d0014a24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "parameters, grads = update_parameters_test_case()\n",
        "parameters = update_parameters(parameters, grads, 0.1)\n",
        "\n",
        "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
        "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
        "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
        "print (\"b2 = \"+ str(parameters[\"b2\"]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 = tensor([[ 0.3119692206, -0.2410473526, -0.3283980787, -1.1436653137],\n",
            "        [ 1.0398452282, -0.4964017868,  0.5393572450,  0.4398358464],\n",
            "        [-0.2483879030, -1.1404219866,  1.2664123774,  0.6238473058]])\n",
            "b1 = tensor([[-1.2400170565],\n",
            "        [-1.2711932659],\n",
            "        [-0.1447267979]])\n",
            "W2 = tensor([[-0.9417777061,  0.5254638195, -0.3200427592]])\n",
            "b2 = tensor([[-0.0250098575]])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}